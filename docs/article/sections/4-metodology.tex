\section{Metodologia}
\label{sec:metodologia}

Esta seção descreve como o estudo foi conduzido, desde o planejamento dos objetivos de nível de serviço (SLOs) até a coleta e interpretação dos experimentos. O enfoque é aplicado e experimental: toda a instrumentação foi construída diretamente no repositório ValorizeAI, o que permite a reprodução dos resultados.

\subsection{Tipo de Pesquisa e Estratégia Geral}

O trabalho caracteriza-se como uma \textbf{pesquisa aplicada} conduzida como \textbf{estudo de caso} de um sistema real em produção. A estratégia seguiu quatro fases iterativas:
\begin{enumerate}
    \item \textbf{Planejamento}: definição dos SLOs, premissas de carga e limites de infraestrutura, além da modelagem arquitetural (documentadas em \texttt{docs/planning.md} e \texttt{docs/system-design.md}).
    \item \textbf{Preparação do ambiente}: uso de Terraform, Docker e Makefile para garantir que infraestrutura, pipelines e dados sintéticos pudessem ser recriados em qualquer momento.
    \item \textbf{Execução controlada}: aplicação de testes de carga com k6 e de processamento assíncrono com Cloud Tasks diretamente sobre a arquitetura final implantada no Google Cloud.
    \item \textbf{Coleta e análise}: armazenamento das métricas e geração de relatórios (\texttt{docs/test-results.md}, \texttt{docs/tests/3-test-queue/README.md}) que subsidiam os capítulos de implementação e resultados.
\end{enumerate}

\subsection{Arquitetura do Ambiente Experimental}

A Figura \ref{fig:arquitetura} sintetiza os componentes usados nos experimentos. O tráfego HTTP/HTTPS entra por um \textbf{Cloud Load Balancer} com \textbf{Cloud CDN}, que reduz a latência de \textit{assets} estáticos e protege o backend com inspeção WAF. Esse tráfego é encaminhado para dois serviços Cloud Run:
\begin{itemize}
    \item \textbf{API Laravel}: processa requisições REST, expõe endpoints usados pelos testes k6 e orquestra o pipeline assíncrono.
    \item \textbf{Laravel Reverb}: mantém conexões WebSocket persistentes para eventos em tempo real; é tratado como serviço independente para permitir escalonamento específico.
\end{itemize}

Ambos os serviços acessam o \textbf{Memorystore for Redis}, usado simultaneamente como cache de leitura (padrão \textit{cache-aside}) e como \textit{backplane} Pub/Sub do Reverb. O armazenamento transacional permanece no \textbf{Cloud SQL for PostgreSQL}, que atende às operações de leitura e escrita executadas durante os testes. Para workloads assíncronos, a API publica tarefas em \textbf{Cloud Tasks}, que aciona workers HTTP também hospedados no Cloud Run. Artefatos grandes (extratos e relatórios) são persistidos no \textbf{Cloud Storage}, mas não fizeram parte dos testes de carga.

\begin{figure}[ht]
    \centering
    \resizebox{\linewidth}{!}{%
    \begin{tikzpicture}[node distance=1.5cm, every node/.style={font=\footnotesize, align=center}]
        \node (cdn) [draw, rounded corners, fill=gray!15, minimum width=5cm, minimum height=0.9cm] {Cloud Load Balancer + Cloud CDN};
        \node (api) [draw, rounded corners, fill=blue!10, minimum width=3cm, minimum height=0.9cm, below left=1.1cm and 2.0cm of cdn] {Cloud Run\\API};
        \node (reverb) [draw, rounded corners, fill=blue!10, minimum width=3cm, minimum height=0.9cm, below=1.1cm of cdn] {Cloud Run\\Reverb};
        \node (workers) [draw, rounded corners, fill=blue!10, minimum width=3cm, minimum height=0.9cm, below right=1.1cm and 2.0cm of cdn] {Cloud Run\\Workers};
        \node (shared) [draw, rounded corners, fill=orange!15, minimum width=6cm, minimum height=1.2cm, below=1.3cm of reverb] {Cloud SQL + Memorystore (Redis) + Cloud Storage};
        \node (tasks) [draw, rounded corners, fill=green!10, minimum width=5cm, minimum height=0.9cm, below=1.0cm of shared] {Cloud Tasks};

        \draw[->, thick] (cdn) -- (api);
        \draw[->, thick] (cdn) -- (reverb);
        \draw[->, thick] (cdn) -- (workers);
        \draw[->, thick] (api) -- (shared);
        \draw[->, thick] (reverb) -- (shared);
        \draw[->, thick] (workers) -- (shared);
        \draw[->, thick] (api) |- (tasks);
        \draw[->, thick] (tasks) -| (workers);
    \end{tikzpicture}}
    \caption{Arquitetura utilizada nos experimentos.}
    \label{fig:arquitetura}
\end{figure}

\subsection{Ferramentas e Processo de Preparação}

Do ponto de vista de engenharia, três pilares garantiram a reprodutibilidade:
\textbf{(i)} \emph{Infraestrutura como Código}: os módulos Terraform descrevem VPC, balanceadores, Cloud Run, Cloud SQL, Redis e Cloud Tasks. Cada mudança passa por \textit{plan/apply} versionado, evitando deriva de ambiente.
\textbf{(ii) Ambientes determinísticos}: o Makefile e os manifestos Docker recompõem o stack local (PostgreSQL, Redis, Loki/Tempo e PHP 8.4) idêntico ao ambiente de teste antes de qualquer execução k6.
\textbf{(iii) Observabilidade}: OpenTelemetry + Cloud Monitoring coletam métricas de latência, uso CPU/memória e backlog de filas, permitindo correlacionar cada rodada com os SLOs definidos.

\subsection{Planejamento dos SLOs e Desenho dos Cenários}

Com base nas premissas de negócio e na literatura de SRE \cite{mccoy_slo_2020,google_sre_book_main}, o sistema foi avaliado contra três metas: latência P95 $\leq 250$~ms, taxa de erro $<$ 0{,}5\% e disponibilidade mensal $\geq 99{,}5\%$. A cota do ambiente (instâncias Cloud Run com 1~vCPU/1~GiB e \texttt{max-instances=10}) estabelece o teto de aproximadamente 900 requisições por segundo; os testes foram configurados para saturar esse limite de maneira controlada.

Dois cenários foram modelados:
\begin{enumerate}
    \item \textbf{Leitura intensiva}: 1{.}000 usuários virtuais consultando listas de transações por 17 minutos em seis estágios, exercitando cache Redis + réplica de leitura do PostgreSQL.
    \item \textbf{Mistura leitura/escrita}: 650 usuários virtuais alternando consultas e criação de transações durante 21 minutos, forçando locks no banco e pressionando o pipeline de escrita.
\end{enumerate}
Além desses ensaios HTTP, foi planejado um \textbf{teste de filas} no qual 51{,}58~mil tarefas artificialmente geradas percorrem o fluxo Cloud Tasks → workers HTTP, mensurando o tempo de drenagem e a elasticidade dos workers.

\subsection{Execução dos Experimentos}

Cada rodada segue os passos:
\begin{enumerate}
    \item \textbf{Preparação dos dados}: seeds e factories povoam o PostgreSQL com contas, transações e orçamentos realistas; a instância Redis é pre-aquecida com métricas e dashboards frequentes.
    \item \textbf{Disparo do cenário}: os scripts k6 (\texttt{transactions-list.js} e \texttt{mix.js}) são executados via Makefile, apontando para o domínio público do Cloud Load Balancer; parâmetros como estágios, VUs e SLIs monitorados são idênticos aos descritos em \texttt{docs/test-results.md}.
    \item \textbf{Registro automático}: os resultados agregados são gravados em CSVs (latência, taxa de erro, uso de VUs) e resumidos no relatório textual. Paralelamente, Cloud Monitoring marca o período da execução para correlacionar com métricas de infraestrutura.
    \item \textbf{Teste de filas}: um script HTTP enfileira 51{,}58~mil mensagens; a drenagem é observada via Cloud Tasks e logs dos workers (descrita em \texttt{docs/tests/3-test-queue/README.md}).
\end{enumerate}

Os experimentos mostraram que o cenário de leitura manteve P95 em 158{,}48~ms e throughput médio de 470 req/s, enquanto o cenário misto violou o SLO (P95 de 4{,}03~s), apontando gargalos de escrita. O teste de filas drenou todo o backlog em aproximadamente 10 minutos sem DLQ, confirmando a resiliência do pipeline assíncrono para o volume simulado.

\subsection{Coleta e Integração das Evidências}

As evidências produzidas alimentam os capítulos seguintes:
\begin{itemize}
    \item \textbf{Planilhas de latência e throughput}: derivadas dos CSVs exportados pelo k6, utilizadas na análise do Capítulo 6 para comparar métricas observadas com os SLOs.
    \item \textbf{Series temporais de infraestrutura}: capturas dos dashboards do Cloud Monitoring documentam uso de CPU das instâncias Cloud Run, saturação do Redis e backlog do Cloud Tasks durante cada rodada.
    \item \textbf{Relato textual}: \texttt{docs/test-results.md} e \texttt{docs/tests/3-test-queue/README.md} consolidam horários, parâmetros e observações, funcionando como diário de laboratório.
\end{itemize}

Essa metodologia garante rastreabilidade completa entre arquitetura (Capítulo \ref{sec:fundamentacao}), implementação (Capítulo 5) e resultados (Capítulo 6), pois cada passo experimental está ancorado em artefatos versionados do projeto.
